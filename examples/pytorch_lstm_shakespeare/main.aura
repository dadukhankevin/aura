imports {
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import os
    import requests
    import numpy as np
}

# --- Flat Configuration Descriptions ---
desc data_url "URL for Shakespeare dataset Make one up that is correct."
desc seq_length "Training sequence length: 100"
desc batch_size "Batch size: 64"
desc embedding_dim "Embedding size: 256 should be used"
desc rnn_units "LSTM units: 1024"
desc epochs "Training epochs: 10"
desc checkpoint_dir "Checkpoint path: ./training_checkpoints_aura"
desc start_string "Generation seed: 'JULIET: '"
desc num_generate "Generate 300 characters"
desc temperature "Sampling temp: 0.7"

# --- Atomic Processing Steps ---
desc step_download "Download text using the @data_url" placing it into --url first
desc step_vocab "Create char->int mappings"
desc step_sequences "Convert text to training sequences"
desc step_batching """
Batch Data Preparation using PyTorch:
1. Take the 1D tensor of integer character IDs.
2. Determine the batch size (e.g., 64) and sequence length (e.g., 100) from context.
3. Calculate the number of batches that can be formed (`total_length // (batch_size * seq_length)`).
4. Calculate the total number of characters to keep (`num_batches * batch_size * seq_length`).
5. Trim the data tensor to only include these characters.
6. Reshape the trimmed data into `(batch_size, -1)`.
7. Transpose the tensor dimensions to get the shape `(sequence_length, batch_size)`.
8. Ensure the final tensor is contiguous in memory. This is the final dataset structure.
"""

desc model_embedding "Embedding layer with @embedding_dim dimensions"
desc model_lstm "PyTorch nn.LSTM layer with @rnn_units units"
desc training_flow "Train for @epochs epochs with Adam optimizer"
desc checkpointing "Save model to @checkpoint_dir"

# --- Core Implementation ---
def --prepare_data() -> (text, vocab, char2idx, idx2char, dataset):
    """Process raw text into training data"""
    Execute @step_download
    Perform @step_vocab on downloaded text
    Apply @step_sequences to create training examples as integer sequences
    # Reference the consolidated, detailed batching description
    Implement @step_batching on the integer sequences to produce the final --dataset
    Return processed components

def --build_model(--vocab_size: int) -> model:
    """Construct LSTM architecture"""
    Create @model_embedding for input size --vocab_size
    Add @model_lstm for sequence processing
    Include linear output layer
    Return complete --model

def --train_model(--model, --dataset) -> trained_model:
    """Execute training pipeline"""
    Configure @training_flow parameters
    Implement @checkpointing strategy
    Return optimized --trained_model

# --- Generation Flow ---
def --generate_text(--model, --char_mappings) -> output:
    """
    Generates text character by character using the trained model.
    
    Starting with the seed text @start_string, predict the next character, 
    append it, and use that prediction as input for the next step.
    Repeat this process to generate @num_generate characters in total.
    Use the @temperature setting to control the randomness of predictions.
    Ensure character IDs are properly converted to the format the model expects at each step.
    """
    # Initialize the process using the start string and character mappings
    --generated_text = iteratively predict and append characters:
        Feed the current sequence (as appropriate model input) to the --model
        Sample the next character ID from the model's output using @temperature
        Convert the ID back to a character and add it to the result
        Update the sequence for the next prediction
        Stop after generating @num_generate characters
    Return the complete --generated_text as the final --output string.

# --- Aura-Style Main Definition ---
def --main():
    """Orchestrate pipeline using high-level prompts returns none"""
    Execute data preparation phase:
        1. Fetch and process text from @data_url Use the full URL.
        2. Create character mappings via @step_vocab
        3. Generate training batches using @batch_size
    
    Execute model training phase:
        1. Build LSTM with @embedding_dim and @rnn_units
        2. Train for @epochs epochs
        3. Save checkpoints to @checkpoint_dir
    
    Execute text generation phase:
        1. Load best model checkpoint
        2. Seed with @start_string
        3. Generate @num_generate characters
        4. Apply @temperature sampling
    
    Display final generated text with header

# --- Pure Python Execution ---
if __name__ == "__main__":
    main()